{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3c888cb",
   "metadata": {},
   "source": [
    "# Clickbait Detection: Multimodal Feature Extraction Pipeline\n",
    "\n",
    "## Overview\n",
    "Build a comprehensive training dataset for clickbait detection/neutralization that merges:\n",
    "- **Casual submissions** (labeled as non-clickbait, ~14.7k unique videos)\n",
    "- **Titles** (356k+ submissions, merged with votes for clickbait scoring)\n",
    "- **Thumbnails** (~28% of casual videos, with image features)\n",
    "- **Metadata** (views, category, publish date, upvotes)\n",
    "\n",
    "### Key Insights\n",
    "- Text-only: 14.7k pairs from casual submissions\n",
    "- Multimodal: ~4,100 pairs with both text + image (28% coverage)\n",
    "- Semi-supervised challenge: 2% of thumbnails labeled â†’ leverage unlabeled pool\n",
    "- Language distribution: ~97% EN, 1% IT, 0.5% RU, 1.5% other_romance\n",
    "\n",
    "### Output\n",
    "A labeled dataset for training:\n",
    "- Text-only models (LLMs fine-tuning for neutralization)\n",
    "- Multimodal models (vision-language transformers for clickbait detection)\n",
    "- Semi-supervised learning on unlabeled submission pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50ddf959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Core packages listed. Uncomment above to install all, or use notebook_install_packages tool.\n"
     ]
    }
   ],
   "source": [
    "# Install required libraries (run once)\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "packages = [\n",
    "    'pandas', 'numpy', 'pillow', 'opencv-python', 'pytesseract',\n",
    "    'langdetect', 'textblob', 'transformers', 'torch', 'torchvision',\n",
    "    'matplotlib', 'seaborn', 'plotly'\n",
    "]\n",
    "\n",
    "# Uncomment to install all at once\n",
    "# for pkg in packages:\n",
    "#     subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', pkg])\n",
    "\n",
    "print(\"âœ“ Core packages listed. Uncomment above to install all, or use notebook_install_packages tool.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2cb303d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PIL/opencv not installed. Install with: pip install pillow opencv-python\n",
      "Data directory: True \n",
      "Current working dir: c:\\Users\\Sahar\\Desktop\\Clickbait_git\\Clickbait-project\\EDA\n"
     ]
    }
   ],
   "source": [
    "# Imports and Setup\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "import json\n",
    "\n",
    "# NLP and Language Detection\n",
    "try:\n",
    "    from langdetect import detect, detect_langs\n",
    "except ImportError:\n",
    "    print(\"langdetect not installed. Install with: pip install langdetect\")\n",
    "    \n",
    "try:\n",
    "    from textblob import TextBlob\n",
    "except ImportError:\n",
    "    print(\"textblob not installed. Install with: pip install textblob\")\n",
    "\n",
    "# Image Processing\n",
    "try:\n",
    "    from PIL import Image\n",
    "    import cv2\n",
    "except ImportError:\n",
    "    print(\"PIL/opencv not installed. Install with: pip install pillow opencv-python\")\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Data directory\n",
    "DATA_DIR = Path('../deArrow_data')\n",
    "print('Data directory:', DATA_DIR.exists(), '\\nCurrent working dir:', Path.cwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf76492a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Data Loaded ===\n",
      "casual_votes: 21590 rows, 14747 unique videos\n",
      "casual_titles: 12571 rows, 12426 unique videos\n",
      "titles: 493305 rows, 356427 unique videos\n",
      "thumbnails: 182406 rows, 162827 unique videos\n",
      "video_info: 9846585 rows\n",
      "\n",
      "Casual (non-clickbait) videoIDs: 14747\n",
      "\n",
      "Merged dataset: 549330 rows\n",
      "  - With thumbnail_UUID (multimodal): 263569 (48.0%)\n",
      "  - Labeled casual (non-clickbait): 31053\n",
      "  - Unlabeled (potential clickbait or semi-supervised): 518277\n"
     ]
    }
   ],
   "source": [
    "# 1. Load Core Data with Merging\n",
    "def load_and_merge_data():\n",
    "    # Load casual submissions (labeled as non-clickbait)\n",
    "    casual_votes = pd.read_csv(DATA_DIR / 'casualVotes.csv')\n",
    "    casual_titles = pd.read_csv(DATA_DIR / 'casualVoteTitles.csv')\n",
    "    \n",
    "    # Load titles with votes (clickbait scoring)\n",
    "    titles = pd.read_csv(DATA_DIR / 'titles.csv')\n",
    "    title_votes = pd.read_csv(DATA_DIR / 'titleVotes.csv')\n",
    "    \n",
    "    # Load thumbnails (image data reference)\n",
    "    thumbnails = pd.read_csv(DATA_DIR / 'thumbnails.csv')\n",
    "    \n",
    "    # Load video metadata\n",
    "    video_info = pd.read_csv(DATA_DIR / 'videoInfo.csv')\n",
    "    \n",
    "    print(\"=== Data Loaded ===\")\n",
    "    print(f\"casual_votes: {len(casual_votes)} rows, {casual_votes['videoID'].nunique()} unique videos\")\n",
    "    print(f\"casual_titles: {len(casual_titles)} rows, {casual_titles['videoID'].nunique()} unique videos\")\n",
    "    print(f\"titles: {len(titles)} rows, {titles['videoID'].nunique()} unique videos\")\n",
    "    print(f\"thumbnails: {len(thumbnails)} rows, {thumbnails['videoID'].nunique()} unique videos\")\n",
    "    print(f\"video_info: {len(video_info)} rows\")\n",
    "    \n",
    "    # Merge titles with title_votes for vote scores\n",
    "    titles_with_votes = titles.merge(\n",
    "        title_votes[['UUID', 'votes']], \n",
    "        on='UUID', \n",
    "        how='left'\n",
    "    ).fillna({'votes': 0})\n",
    "    \n",
    "    # Identify casual videos: videos with casual_votes (non-clickbait ground truth)\n",
    "    casual_video_ids = set(casual_votes['videoID'].dropna().unique())\n",
    "    print(f\"\\nCasual (non-clickbait) videoIDs: {len(casual_video_ids)}\")\n",
    "    \n",
    "    # Create base dataset: all videos with titles\n",
    "    dataset = titles_with_votes[['videoID', 'title', 'UUID', 'votes', 'original', 'casualMode']].copy()\n",
    "    \n",
    "    # Add casual label (1 = non-clickbait from casual submissions, 0 = potential clickbait or unlabeled)\n",
    "    dataset['is_casual'] = dataset['videoID'].isin(casual_video_ids).astype(int)\n",
    "    \n",
    "    # Merge with thumbnails (28% coverage)\n",
    "    dataset = dataset.merge(\n",
    "        thumbnails[['videoID', 'UUID', 'original', 'timeSubmitted']].rename(\n",
    "            columns={'UUID': 'thumbnail_UUID', 'original': 'is_original_thumbnail', 'timeSubmitted': 'thumbnail_time'}\n",
    "        ),\n",
    "        on='videoID',\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Add video metadata (videoInfo only has videoID, channelID, title, published)\n",
    "    dataset = dataset.merge(\n",
    "        video_info[['videoID', 'channelID', 'published']].drop_duplicates(subset=['videoID']),\n",
    "        on='videoID',\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nMerged dataset: {len(dataset)} rows\")\n",
    "    print(f\"  - With thumbnail_UUID (multimodal): {dataset['thumbnail_UUID'].notna().sum()} ({100*dataset['thumbnail_UUID'].notna().sum()/len(dataset):.1f}%)\")\n",
    "    print(f\"  - Labeled casual (non-clickbait): {dataset['is_casual'].sum()}\")\n",
    "    print(f\"  - Unlabeled (potential clickbait or semi-supervised): {len(dataset) - dataset['is_casual'].sum()}\")\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "dataset = load_and_merge_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9a2784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detecting languages (this may take a minute)...\n"
     ]
    }
   ],
   "source": [
    "# 2. Language Detection Filtering\n",
    "def detect_language_safe(text):\n",
    "    \"\"\"Detect language with fallback to 'unknown'\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return 'unknown'\n",
    "    try:\n",
    "        return detect(str(text)[:500])  # Use first 500 chars for speed\n",
    "    except:\n",
    "        return 'unknown'\n",
    "\n",
    "print(\"Detecting languages (this may take a minute)...\")\n",
    "dataset['language'] = dataset['title'].apply(detect_language_safe)\n",
    "\n",
    "lang_dist = dataset['language'].value_counts()\n",
    "print(\"\\n=== Language Distribution ===\")\n",
    "print(lang_dist)\n",
    "\n",
    "# Focus on EN, IT, ES for now\n",
    "focus_langs = {'en', 'it', 'es'}\n",
    "dataset['language_focus'] = dataset['language'].isin(focus_langs).astype(int)\n",
    "\n",
    "print(f\"\\nFocused languages (EN, IT, ES): {dataset['language_focus'].sum()} rows ({100*dataset['language_focus'].sum()/len(dataset):.1f}%)\")\n",
    "print(f\"Proceeding with focused language set for feature extraction...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d536ccef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Text Feature Extraction\n",
    "def extract_text_features(text):\n",
    "    \"\"\"Extract linguistic and sentiment features\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return {\n",
    "            'title_length': 0, 'word_count': 0, 'char_per_word': 0,\n",
    "            'exclamation_count': 0, 'question_count': 0, 'ellipsis_count': 0,\n",
    "            'uppercase_ratio': 0, 'digit_count': 0, 'emoji_count': 0,\n",
    "            'caps_words': 0, 'has_caps_sequence': 0,\n",
    "            'punctuation_density': 0, 'sentiment_polarity': 0, 'sentiment_subjectivity': 0\n",
    "        }\n",
    "    \n",
    "    text_str = str(text).strip()\n",
    "    words = text_str.split()\n",
    "    \n",
    "    # Sentiment analysis\n",
    "    try:\n",
    "        blob = TextBlob(text_str)\n",
    "        sentiment_polarity = blob.sentiment.polarity\n",
    "        sentiment_subjectivity = blob.sentiment.subjectivity\n",
    "    except:\n",
    "        sentiment_polarity = 0\n",
    "        sentiment_subjectivity = 0\n",
    "    \n",
    "    features = {\n",
    "        'title_length': len(text_str),\n",
    "        'word_count': len(words),\n",
    "        'char_per_word': len(text_str) / max(1, len(words)),\n",
    "        'exclamation_count': text_str.count('!'),\n",
    "        'question_count': text_str.count('?'),\n",
    "        'ellipsis_count': text_str.count('...'),\n",
    "        'uppercase_ratio': sum(1 for c in text_str if c.isupper()) / max(1, len(text_str)),\n",
    "        'digit_count': sum(1 for c in text_str if c.isdigit()),\n",
    "        'emoji_count': len(re.findall(r'[ðŸ˜€-ðŸ™ðŸŒ€-ðŸ—¿ðŸš€-ðŸ›¿]', text_str)),  # Simplified emoji detection\n",
    "        'caps_words': sum(1 for w in words if w.isupper() and len(w) > 1),\n",
    "        'has_caps_sequence': int(bool(re.search(r'[A-Z]{3,}', text_str))),\n",
    "        'punctuation_density': sum(1 for c in text_str if c in '!?.,:;-()[]{}') / max(1, len(text_str)),\n",
    "        'sentiment_polarity': sentiment_polarity,\n",
    "        'sentiment_subjectivity': sentiment_subjectivity,\n",
    "    }\n",
    "    return features\n",
    "\n",
    "print(\"Extracting text features...\")\n",
    "text_features = dataset['title'].apply(extract_text_features).apply(pd.Series)\n",
    "dataset = pd.concat([dataset, text_features.add_prefix('text_')], axis=1)\n",
    "\n",
    "print(f\"âœ“ Text features added: {[c for c in dataset.columns if c.startswith('text_')][:5]}...\")\n",
    "print(f\"\\nText feature summary (title length):\")\n",
    "print(dataset['text_title_length'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c6cb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Image Feature Extraction (Metadata-based for now, actual image download optional)\n",
    "def extract_image_features_metadata():\n",
    "    \"\"\"\n",
    "    Extract image-related metadata features.\n",
    "    For actual image processing (brightness, histogram, OCR), load thumbnails via deArrow API.\n",
    "    Here we create placeholders and document the pipeline.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add placeholder columns for image features\n",
    "    image_features = {\n",
    "        'image_brightness_mean': np.nan,  # Would compute from PIL.Image\n",
    "        'image_color_dominant_r': np.nan,  # Color histogram\n",
    "        'image_color_dominant_g': np.nan,\n",
    "        'image_color_dominant_b': np.nan,\n",
    "        'image_face_detected': 0,  # Would use face_recognition library\n",
    "        'image_text_detected': 0,  # Would use pytesseract/EasyOCR\n",
    "        'image_text_confidence': 0.0,\n",
    "    }\n",
    "    \n",
    "    for col, val in image_features.items():\n",
    "        dataset[col] = val\n",
    "    \n",
    "    print(\"âœ“ Image feature columns created (ready for thumbnail download + processing)\")\n",
    "    print(f\"  Columns: {list(image_features.keys())}\")\n",
    "    print(f\"\\nNote: To extract actual image features:\")\n",
    "    print(f\"  1. Download thumbnails from deArrow (videoID â†’ thumbnail URL)\")\n",
    "    print(f\"  2. Use PIL for brightness/color histogram\")\n",
    "    print(f\"  3. Use face_recognition or mediapipe for face detection\")\n",
    "    print(f\"  4. Use pytesseract or EasyOCR for text detection\")\n",
    "    print(f\"  5. Integrate CLIP for image-text matching score\")\n",
    "\n",
    "extract_image_features_metadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be36d765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Metadata Merging and Labeling\n",
    "print(\"=== Metadata Integration ===\")\n",
    "\n",
    "# Clean views and ensure numeric\n",
    "dataset['views'] = pd.to_numeric(dataset['views'], errors='coerce').fillna(0)\n",
    "\n",
    "# Create target labels\n",
    "# Label 0: casual submissions (non-clickbait ground truth)\n",
    "# Label 1: potential clickbait (high votes â†’ likely clickbait by community)\n",
    "# Label -1: unlabeled (needs semi-supervised approach)\n",
    "\n",
    "def create_label(row):\n",
    "    \"\"\"\n",
    "    Labeling strategy:\n",
    "    - 0: casual submission (ground truth non-clickbait)\n",
    "    - 1: high-voted original submission (likely clickbait)\n",
    "    - -1: other (unlabeled, for semi-supervised learning)\n",
    "    \"\"\"\n",
    "    if row['is_casual'] == 1:\n",
    "        return 0  # Non-clickbait\n",
    "    elif row['original'] == 1 and row['votes'] > 2:\n",
    "        return 1  # Clickbait (highly voted original titles)\n",
    "    else:\n",
    "        return -1  # Unlabeled\n",
    "\n",
    "dataset['label'] = dataset.apply(create_label, axis=1)\n",
    "\n",
    "label_dist = dataset['label'].value_counts().sort_index()\n",
    "print(f\"\\nLabel Distribution:\")\n",
    "for lbl in [-1, 0, 1]:\n",
    "    count = label_dist.get(lbl, 0)\n",
    "    pct = 100 * count / len(dataset)\n",
    "    label_name = ['Unlabeled (semi-supervised)', 'Non-clickbait (casual)', 'Clickbait (voted)'][lbl + 1]\n",
    "    print(f\"  {lbl}: {count:>6} ({pct:>5.1f}%) - {label_name}\")\n",
    "\n",
    "# Multimodal coverage\n",
    "print(f\"\\n=== Modality Coverage ===\")\n",
    "print(f\"Text-only (no thumbnail): {(~dataset['thumbnail_UUID'].notna()).sum()} rows\")\n",
    "print(f\"Multimodal (text + image): {dataset['thumbnail_UUID'].notna().sum()} rows\")\n",
    "print(f\"  - Non-clickbait (text + image): {dataset[(dataset['label']==0) & (dataset['thumbnail_UUID'].notna())].shape[0]}\")\n",
    "print(f\"  - Clickbait (text + image): {dataset[(dataset['label']==1) & (dataset['thumbnail_UUID'].notna())].shape[0]}\")\n",
    "\n",
    "# Show sample\n",
    "print(f\"\\n=== Sample Labeled Data ===\")\n",
    "print(dataset[['title', 'is_casual', 'language', 'votes', 'views', 'label']].head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707d8313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Data Balance and Semi-Supervised Planning\n",
    "print(\"=== Data Analysis for Model Training ===\\n\")\n",
    "\n",
    "# Text-only dataset (labeled)\n",
    "text_only_labeled = dataset[(dataset['label'] >= 0)]\n",
    "print(f\"TEXT-ONLY DATASET:\")\n",
    "print(f\"  Total: {len(text_only_labeled)} rows\")\n",
    "print(f\"  Non-clickbait: {(text_only_labeled['label']==0).sum()}\")\n",
    "print(f\"  Clickbait: {(text_only_labeled['label']==1).sum()}\")\n",
    "print(f\"  Class ratio (non-clickbait:clickbait): {(text_only_labeled['label']==0).sum() / max(1, (text_only_labeled['label']==1).sum()):.2f}:1\")\n",
    "\n",
    "# Multimodal dataset (labeled)\n",
    "multimodal_labeled = dataset[(dataset['label'] >= 0) & (dataset['thumbnail_UUID'].notna())]\n",
    "print(f\"\\nMULTIMODAL DATASET (text + image):\")\n",
    "print(f\"  Total: {len(multimodal_labeled)} rows\")\n",
    "print(f\"  Non-clickbait: {(multimodal_labeled['label']==0).sum()}\")\n",
    "print(f\"  Clickbait: {(multimodal_labeled['label']==1).sum()}\")\n",
    "if (multimodal_labeled['label']==1).sum() > 0:\n",
    "    print(f\"  Class ratio: {(multimodal_labeled['label']==0).sum() / (multimodal_labeled['label']==1).sum():.2f}:1\")\n",
    "\n",
    "# Unlabeled pool (for semi-supervised)\n",
    "unlabeled = dataset[dataset['label'] == -1]\n",
    "print(f\"\\nUNLABELED POOL (semi-supervised learning):\")\n",
    "print(f\"  Total: {len(unlabeled)} rows ({100*len(unlabeled)/len(dataset):.1f}%)\")\n",
    "print(f\"  Unlabeled with thumbnails: {(unlabeled['thumbnail_UUID'].notna()).sum()}\")\n",
    "\n",
    "# Language breakdown\n",
    "print(f\"\\nLANGUAGE BREAKDOWN (focused EN, IT, ES):\")\n",
    "lang_subset = dataset[dataset['language_focus'] == 1]\n",
    "for lang in ['en', 'it', 'es']:\n",
    "    lang_count = (lang_subset['language'] == lang).sum()\n",
    "    print(f\"  {lang.upper()}: {lang_count} ({100*lang_count/len(lang_subset):.1f}%)\")\n",
    "\n",
    "# Visualization: Class balance across modalities\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Text-only\n",
    "text_labels = text_only_labeled['label'].value_counts()\n",
    "axes[0].bar(['Non-CB', 'CB'], [text_labels.get(0, 0), text_labels.get(1, 0)], color=['green', 'red', 'gray'][:2])\n",
    "axes[0].set_title('Text-Only (Labeled)')\n",
    "axes[0].set_ylabel('Count')\n",
    "\n",
    "# Multimodal\n",
    "if len(multimodal_labeled) > 0:\n",
    "    mm_labels = multimodal_labeled['label'].value_counts()\n",
    "    axes[1].bar(['Non-CB', 'CB'], [mm_labels.get(0, 0), mm_labels.get(1, 0)], color=['green', 'red'][:len(mm_labels)])\n",
    "    axes[1].set_title('Multimodal (Labeled)')\n",
    "else:\n",
    "    axes[1].text(0.5, 0.5, 'No multimodal data', ha='center', va='center', transform=axes[1].transAxes)\n",
    "    axes[1].set_title('Multimodal (Labeled)')\n",
    "\n",
    "# Unlabeled\n",
    "axes[2].bar(['Unlabeled'], [len(unlabeled)], color='gray', alpha=0.5)\n",
    "axes[2].set_title(f'Unlabeled Pool ({100*len(unlabeled)/len(dataset):.0f}%)')\n",
    "axes[2].set_ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('data_balance.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nâœ“ Saved visualization to data_balance.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0f6583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Save Labeled Datasets and Feature Matrix\n",
    "import json\n",
    "\n",
    "# Select feature columns for output\n",
    "feature_cols = [c for c in dataset.columns if c.startswith('text_')]\n",
    "output_cols = [\n",
    "    'videoID', 'title', 'language', 'label', 'is_casual', 'votes', 'views',\n",
    "    'thumbnail_UUID', 'UUID'\n",
    "] + feature_cols\n",
    "\n",
    "# Text-only (labeled)\n",
    "text_only_labeled_out = text_only_labeled[output_cols].copy()\n",
    "text_only_labeled_out.to_csv('training_data_text_only.csv', index=False)\n",
    "print(f\"âœ“ Saved text-only training data: {len(text_only_labeled_out)} rows to training_data_text_only.csv\")\n",
    "\n",
    "# Multimodal (labeled)\n",
    "if len(multimodal_labeled) > 0:\n",
    "    multimodal_out = multimodal_labeled[output_cols].copy()\n",
    "    multimodal_out.to_csv('training_data_multimodal.csv', index=False)\n",
    "    print(f\"âœ“ Saved multimodal training data: {len(multimodal_out)} rows to training_data_multimodal.csv\")\n",
    "\n",
    "# Unlabeled pool (for semi-supervised)\n",
    "unlabeled_out = unlabeled[output_cols].copy()\n",
    "unlabeled_out.to_csv('unlabeled_pool.csv', index=False)\n",
    "print(f\"âœ“ Saved unlabeled pool: {len(unlabeled_out)} rows to unlabeled_pool.csv\")\n",
    "\n",
    "# Full feature matrix\n",
    "dataset_out = dataset[output_cols].copy()\n",
    "dataset_out.to_csv('training_data_full.csv', index=False)\n",
    "print(f\"âœ“ Saved full feature matrix: {len(dataset_out)} rows to training_data_full.csv\")\n",
    "\n",
    "# Save summary metadata\n",
    "metadata = {\n",
    "    'dataset_name': 'deArrow Clickbait Detection',\n",
    "    'total_samples': len(dataset),\n",
    "    'text_only_labeled': len(text_only_labeled),\n",
    "    'multimodal_labeled': len(multimodal_labeled),\n",
    "    'unlabeled': len(unlabeled),\n",
    "    'language_focus': ['en', 'it', 'es'],\n",
    "    'text_features': feature_cols,\n",
    "    'image_features_placeholder': [\n",
    "        'image_brightness_mean', 'image_color_dominant_r', 'image_color_dominant_g',\n",
    "        'image_color_dominant_b', 'image_face_detected', 'image_text_detected'\n",
    "    ],\n",
    "    'labels': {\n",
    "        '0': 'Non-clickbait (casual submissions)',\n",
    "        '1': 'Clickbait (voted originals)',\n",
    "        '-1': 'Unlabeled (semi-supervised)'\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('training_metadata.json', 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(f\"\\nâœ“ Saved metadata to training_metadata.json\")\n",
    "print(f\"\\nDataset Summary:\")\n",
    "print(json.dumps(metadata, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1715c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Model Training Recommendations\n",
    "print(\"=\" * 80)\n",
    "print(\"MODEL TRAINING STRATEGIES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "recommendations = \"\"\"\n",
    "### 1. TEXT-ONLY MODELS (14.7k+ samples)\n",
    "   **Architecture:** Fine-tune pre-trained LLM on clickbait neutralization\n",
    "   \n",
    "   **Datasets:**\n",
    "   - Supervised: ~14.7k casual (non-clickbait) vs. ~7k voted originals\n",
    "   - Semi-supervised: 350k+ unlabeled submissions\n",
    "   \n",
    "   **Recommended Approaches:**\n",
    "   a) Fine-tune on task: Seq2Seq (T5, BART) for clickbait â†’ neutral generation\n",
    "   b) Classification: RoBERTa, DistilBERT for binary clickbait detection\n",
    "   c) Semi-supervised: Pseudo-labeling on unlabeled pool using uncertainty sampling\n",
    "   \n",
    "   **Libraries:** transformers, torch, pytorch-lightning\n",
    "\n",
    "### 2. MULTIMODAL MODELS (4.1k+ samples with images)\n",
    "   **Architecture:** Vision-Language transformers (CLIP, ViLBERT, LXMERT)\n",
    "   \n",
    "   **Datasets:**\n",
    "   - Text + image pairs: ~4.1k labeled (only ~28% coverage)\n",
    "   - Image-only features: brightness, color histogram, face/text detection\n",
    "   \n",
    "   **Recommended Approaches:**\n",
    "   a) Joint embedding: Use CLIP to learn shared text-image space\n",
    "   b) Classification: Fine-tune ViLBERT for multimodal clickbait detection\n",
    "   c) Data augmentation: Generate synthetic negatives from unlabeled images\n",
    "   \n",
    "   **Libraries:** transformers, timm, torchvision, CLIP\n",
    "   \n",
    "### 3. SEMI-SUPERVISED STRATEGY (350k unlabeled)\n",
    "   **Challenge:** Only 2% of thumbnails labeled â†’ exploit unlabeled pool\n",
    "   \n",
    "   **Approaches:**\n",
    "   a) Pseudo-labeling: Use confident model predictions to label unlabeled data\n",
    "   b) Self-training: Iteratively train on pseudo-labeled pool + real labels\n",
    "   c) Contrastive learning: Learn representations from unlabeled pool (SimCLR)\n",
    "   d) Consistency regularization: MixMatch, FixMatch for image/text\n",
    "   \n",
    "   **Libraries:** semi-supervised-learning libraries, transformers\n",
    "\n",
    "### 4. HANDLING CLASS IMBALANCE\n",
    "   **Issue:** Non-clickbait heavily outweighs clickbait in labeled set\n",
    "   \n",
    "   **Solutions:**\n",
    "   a) Weighted loss: BCE with class weights\n",
    "   b) Focal loss: Focus on hard negative examples\n",
    "   c) Oversampling: SMOTE for synthetic clickbait examples\n",
    "   d) Threshold tuning: Adjust decision boundary for F1/precision\n",
    "   \n",
    "   **Libraries:** scikit-learn, torch, imbalanced-learn\n",
    "\n",
    "### 5. FEATURE ENGINEERING ENHANCEMENTS\n",
    "   **Image features to extract:**\n",
    "   - Brightness, saturation, contrast (PIL/cv2)\n",
    "   - Dominant colors, color entropy (histogram)\n",
    "   - Face detection + count (face_recognition, mediapipe)\n",
    "   - Text detection + OCR (pytesseract, EasyOCR)\n",
    "   - Object detection (YOLOv5) for sensational imagery\n",
    "   \n",
    "   **Libraries:** opencv-python, PIL, face_recognition, pytesseract, yolov5\n",
    "\n",
    "### 6. EVALUATION METRICS\n",
    "   - Precision, Recall, F1 (imbalanced data)\n",
    "   - AUC-ROC (ranking metric)\n",
    "   - Human evaluation on test set (for neutralization task)\n",
    "   - Language-specific performance (EN vs. IT vs. ES)\n",
    "\n",
    "### 7. TRAINING PIPELINE\n",
    "   1. Start with text-only models (faster feedback)\n",
    "   2. Validate on multimodal subset (4.1k samples)\n",
    "   3. Use unlabeled pool for semi-supervised fine-tuning\n",
    "   4. Deploy ensemble (text + multimodal) for robust predictions\n",
    "\"\"\"\n",
    "\n",
    "print(recommendations)\n",
    "\n",
    "# Save recommendations\n",
    "with open('training_recommendations.txt', 'w') as f:\n",
    "    f.write(recommendations)\n",
    "\n",
    "print(\"âœ“ Saved recommendations to training_recommendations.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
