{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f978731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded casualVotes.csv (21,590 rows)\n",
      "Loaded casualVoteTitles.csv (12,571 rows)\n",
      "Loaded categoryVotes.csv (491,806 rows)\n",
      "Loaded lockCategories.csv (163,248 rows)\n",
      "Loaded ratings.csv (9,848 rows)\n",
      "Loaded sponsorTimes.csv (5,231,393 rows)\n",
      "Loaded thumbnails.csv (182,406 rows)\n",
      "Loaded thumbnailTimestamps.csv (157,264 rows)\n",
      "Loaded thumbnailVotes.csv (182,401 rows)\n",
      "Loaded titles.csv (493,305 rows)\n",
      "Loaded titleVotes.csv (493,235 rows)\n",
      "Loaded unlistedVideos.csv (121,403 rows)\n",
      "Loaded userNames.csv (413,388 rows)\n",
      "Loaded videoInfo.csv (9,846,585 rows)\n",
      "Loaded vipUsers.csv (123 rows)\n",
      "Loaded warnings.csv (3,428 rows)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import itertools\n",
    "\n",
    "DATA_DIR = r'C:\\Users\\Sahar\\Desktop\\Clickbait project\\Dataset\\data'\n",
    "\n",
    "def load_data():\n",
    "    data = {}\n",
    "    id_cols = ['videoid', 'channelid', 'userid', 'uuid', 'hashedvideoid']\n",
    "    \n",
    "    for fname in os.listdir(DATA_DIR):\n",
    "        if fname.lower().endswith('.csv'):\n",
    "            fp = os.path.join(DATA_DIR, fname)\n",
    "            try:\n",
    "                df = pd.read_csv(fp, low_memory=False)\n",
    "                df.columns = df.columns.str.lower().str.replace('[^a-z0-9]', '', regex=True)\n",
    "                for c in df.columns:\n",
    "                    if c in id_cols or c.endswith('id') or c.endswith('uuid'):\n",
    "                        df[c] = df[c].astype(str).str.strip()\n",
    "                data[fname] = df\n",
    "                print(f\"Loaded {fname} ({len(df):,} rows)\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {fname}: {e}\")\n",
    "    return data\n",
    "\n",
    "dataset = load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c17e1d03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fast relation analysis (ID columns only, 500 sample)...\n",
      "Analysis completed in 669.7s. Found 19 relations.\n",
      "\n",
      "Results preview:\n",
      "                 File 1      Column 1                File 2      Column 2  Overlap  Coverage          Type\n",
      "0       casualVotes.csv       titleid  casualVoteTitles.csv            id       27    100.00      Identity\n",
      "1       casualVotes.csv       titleid           ratings.csv            id       26     96.30  Intersection\n",
      "2  casualVoteTitles.csv            id           ratings.csv            id       26     96.30  Intersection\n",
      "3    lockCategories.csv        userid        thumbnails.csv        userid       16     17.78  Intersection\n",
      "4    lockCategories.csv        userid            titles.csv        userid       20     22.22  Intersection\n",
      "5    lockCategories.csv        userid          vipUsers.csv        userid       80     88.89  Intersection\n",
      "6    lockCategories.csv        userid          warnings.csv  issueruserid       68     91.89  Intersection\n",
      "7      sponsorTimes.csv        hidden    thumbnailVotes.csv  shadowhidden        2     66.67  Intersection\n",
      "8      sponsorTimes.csv  shadowhidden    thumbnailVotes.csv  shadowhidden        3    100.00      Identity\n",
      "9      sponsorTimes.csv        hidden        titleVotes.csv  shadowhidden        2     66.67  Intersection\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def analyze_relations(data, sample_size=500):\n",
    "    results = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    id_keywords = {'id', 'uuid', 'hash', 'user', 'video', 'channel'}\n",
    "    \n",
    "    for f1, f2 in itertools.combinations(data.keys(), 2):\n",
    "        df1, df2 = data[f1], data[f2]\n",
    "        \n",
    "        id_cols_f1 = [c for c in df1.columns if any(kw in c.lower() for kw in id_keywords)]\n",
    "        id_cols_f2 = [c for c in df2.columns if any(kw in c.lower() for kw in id_keywords)]\n",
    "        \n",
    "        if not id_cols_f1 or not id_cols_f2:\n",
    "            continue\n",
    "        \n",
    "        for c1 in id_cols_f1:\n",
    "            col_data = df1[c1].dropna().unique()\n",
    "            if len(col_data) < 2 or len(col_data) > 100000:\n",
    "                continue\n",
    "                \n",
    "            set1 = set(col_data[:sample_size])\n",
    "            \n",
    "            for c2 in id_cols_f2:\n",
    "                col_data2 = df2[c2].dropna().unique()\n",
    "                if len(col_data2) < 2 or len(col_data2) > 100000:\n",
    "                    continue\n",
    "                    \n",
    "                set2 = set(col_data2[:sample_size])\n",
    "                \n",
    "                intersect = set1.intersection(set2)\n",
    "                if not intersect or len(intersect) < 2:\n",
    "                    continue\n",
    "\n",
    "                len1, len2, len_int = len(set1), len(set2), len(intersect)\n",
    "                smaller = min(len1, len2)\n",
    "                coverage = len_int / smaller * 100 if smaller > 0 else 0\n",
    "\n",
    "                rel_type = None\n",
    "                if len1 == len2 == len_int:\n",
    "                    rel_type = 'Identity'\n",
    "                elif len_int == len1 < len2:\n",
    "                    rel_type = 'Subset A in B'\n",
    "                elif len_int == len2 < len1:\n",
    "                    rel_type = 'Subset A in B'\n",
    "                    f1, f2, c1, c2 = f2, f1, c2, c1\n",
    "                elif coverage >= 5.0:\n",
    "                    rel_type = 'Intersection'\n",
    "                \n",
    "                if rel_type:\n",
    "                    results.append({'File 1': f1, 'Column 1': c1, 'File 2': f2, 'Column 2': c2, \n",
    "                                   'Overlap': len_int, 'Coverage': round(coverage, 2), 'Type': rel_type})\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"Analysis completed in {elapsed:.1f}s. Found {len(results)} relations.\")\n",
    "    return results\n",
    "\n",
    "print(\"Starting fast relation analysis (ID columns only, 500 sample)...\")\n",
    "relations = analyze_relations(dataset, sample_size=500)\n",
    "print(f\"\\nResults preview:\")\n",
    "if relations:\n",
    "    rel_df = pd.DataFrame(relations[:10])\n",
    "    print(rel_df.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeea2e80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e2be0c58",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
