{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e08f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import itertools\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "DATA_DIR = r'C:\\Users\\Sahar\\Desktop\\Clickbait project\\Dataset\\data'\n",
    "\n",
    "def load_data(sample_size=None):\n",
    "    data = {}\n",
    "    id_cols = ['videoid', 'channelid', 'userid', 'uuid', 'hashedvideoid']\n",
    "    \n",
    "    for fname in sorted(os.listdir(DATA_DIR)):\n",
    "        if fname.lower().endswith('.csv'):\n",
    "            fp = os.path.join(DATA_DIR, fname)\n",
    "            try:\n",
    "                df = pd.read_csv(fp, low_memory=False, nrows=sample_size)\n",
    "                df.columns = df.columns.str.lower().str.replace('[^a-z0-9]', '', regex=True)\n",
    "                for c in df.columns:\n",
    "                    if c in id_cols or c.endswith('id') or c.endswith('uuid'):\n",
    "                        df[c] = df[c].astype(str).str.strip()\n",
    "                data[fname] = df\n",
    "                print(f\"Loaded {fname:30s} ({len(df):>10,} rows, {len(df.columns):>3} cols)\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {fname}: {e}\")\n",
    "    return data\n",
    "\n",
    "print(\"Loading dataset...\")\n",
    "dataset = load_data(sample_size=None)\n",
    "print(f\"\\nTotal files loaded: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7228495c",
   "metadata": {},
   "source": [
    "## 1. Data Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1d2d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_data_quality(data):\n",
    "    quality_report = []\n",
    "    \n",
    "    for fname, df in data.items():\n",
    "        missing_pct = (df.isnull().sum() / len(df) * 100).to_dict()\n",
    "        duplicates = df.duplicated().sum()\n",
    "        \n",
    "        quality_report.append({\n",
    "            'File': fname,\n",
    "            'Rows': len(df),\n",
    "            'Columns': len(df.columns),\n",
    "            'Duplicates': duplicates,\n",
    "            'Max_Missing_%': max(missing_pct.values()) if missing_pct else 0,\n",
    "            'Memory_MB': df.memory_usage(deep=True).sum() / 1024**2\n",
    "        })\n",
    "    \n",
    "    quality_df = pd.DataFrame(quality_report)\n",
    "    return quality_df\n",
    "\n",
    "print(\"\\n=== Data Quality Report ===\")\n",
    "quality = assess_data_quality(dataset)\n",
    "print(quality.to_string(index=False))\n",
    "print(f\"\\nTotal Memory Usage: {quality['Memory_MB'].sum():.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c20aed1",
   "metadata": {},
   "source": [
    "## 2. Column Relationship Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0fa2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_column_relationships(data, sample_size=1000):\n",
    "    relationships = []\n",
    "    id_keywords = {'id', 'uuid', 'hash', 'user', 'video', 'channel'}\n",
    "    \n",
    "    for f1, f2 in itertools.combinations(data.keys(), 2):\n",
    "        df1, df2 = data[f1], data[f2]\n",
    "        \n",
    "        id_cols_f1 = [c for c in df1.columns if any(kw in c.lower() for kw in id_keywords)]\n",
    "        id_cols_f2 = [c for c in df2.columns if any(kw in c.lower() for kw in id_keywords)]\n",
    "        \n",
    "        if not id_cols_f1 or not id_cols_f2:\n",
    "            continue\n",
    "        \n",
    "        for c1 in id_cols_f1:\n",
    "            col_data = df1[c1].dropna().unique()[:sample_size]\n",
    "            if len(col_data) < 2 or len(col_data) > 100000:\n",
    "                continue\n",
    "            set1 = set(col_data)\n",
    "            \n",
    "            for c2 in id_cols_f2:\n",
    "                col_data2 = df2[c2].dropna().unique()[:sample_size]\n",
    "                if len(col_data2) < 2 or len(col_data2) > 100000:\n",
    "                    continue\n",
    "                set2 = set(col_data2)\n",
    "                \n",
    "                intersect = set1.intersection(set2)\n",
    "                if not intersect or len(intersect) < 2:\n",
    "                    continue\n",
    "                \n",
    "                len1, len2, len_int = len(set1), len(set2), len(intersect)\n",
    "                coverage = len_int / min(len1, len2) * 100\n",
    "                \n",
    "                rel_type = 'Intersection'\n",
    "                if len1 == len2 == len_int:\n",
    "                    rel_type = 'Identity'\n",
    "                elif len_int == len1 < len2:\n",
    "                    rel_type = 'Subset'\n",
    "                \n",
    "                if coverage >= 5.0:\n",
    "                    relationships.append({\n",
    "                        'From_File': f1,\n",
    "                        'From_Column': c1,\n",
    "                        'To_File': f2,\n",
    "                        'To_Column': c2,\n",
    "                        'Overlap_Count': len_int,\n",
    "                        'Coverage_%': round(coverage, 2),\n",
    "                        'Type': rel_type\n",
    "                    })\n",
    "    \n",
    "    return pd.DataFrame(relationships)\n",
    "\n",
    "print(\"\\nMapping column relationships (ID columns)...\")\n",
    "relationships_df = map_column_relationships(dataset)\n",
    "print(f\"\\nFound {len(relationships_df)} relationships\")\n",
    "print(\"\\nTop 15 Relationships:\")\n",
    "print(relationships_df.nlargest(15, 'Coverage_%')[['From_File', 'From_Column', 'To_File', 'To_Column', 'Coverage_%', 'Type']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d939f0",
   "metadata": {},
   "source": [
    "## 3. Temporal Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a04312",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_temporal_features(data):\n",
    "    temporal_features = []\n",
    "    \n",
    "    for fname, df in data.items():\n",
    "        datetime_cols = []\n",
    "        \n",
    "        for col in df.columns:\n",
    "            if any(keyword in col.lower() for keyword in ['time', 'date', 'timestamp', 'when']):\n",
    "                try:\n",
    "                    pd.to_datetime(df[col], errors='coerce')\n",
    "                    datetime_cols.append(col)\n",
    "                except:\n",
    "                    pass\n",
    "        \n",
    "        if datetime_cols:\n",
    "            for col in datetime_cols:\n",
    "                try:\n",
    "                    dt = pd.to_datetime(df[col], errors='coerce')\n",
    "                    valid_dates = dt.dropna()\n",
    "                    \n",
    "                    if len(valid_dates) > 0:\n",
    "                        temporal_features.append({\n",
    "                            'File': fname,\n",
    "                            'Column': col,\n",
    "                            'Type': 'Datetime',\n",
    "                            'Non_Null_%': (len(valid_dates) / len(df) * 100),\n",
    "                            'Min_Date': valid_dates.min(),\n",
    "                            'Max_Date': valid_dates.max(),\n",
    "                            'Date_Range_Days': (valid_dates.max() - valid_dates.min()).days\n",
    "                        })\n",
    "                except:\n",
    "                    pass\n",
    "    \n",
    "    return pd.DataFrame(temporal_features)\n",
    "\n",
    "print(\"\\nExtracting temporal features...\")\n",
    "temporal_df = extract_temporal_features(dataset)\n",
    "if len(temporal_df) > 0:\n",
    "    print(\"\\nTemporal Columns Found:\")\n",
    "    print(temporal_df[['File', 'Column', 'Non_Null_%', 'Min_Date', 'Max_Date', 'Date_Range_Days']].to_string(index=False))\n",
    "else:\n",
    "    print(\"No temporal columns detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251f3d3d",
   "metadata": {},
   "source": [
    "## 4. Categorical Feature Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347ac62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_categorical_features(data):\n",
    "    categorical_analysis = []\n",
    "    \n",
    "    for fname, df in data.items():\n",
    "        for col in df.select_dtypes(include=['object']).columns:\n",
    "            unique_count = df[col].nunique()\n",
    "            \n",
    "            if unique_count <= 1000:\n",
    "                categorical_analysis.append({\n",
    "                    'File': fname,\n",
    "                    'Column': col,\n",
    "                    'Unique_Values': unique_count,\n",
    "                    'Null_%': (df[col].isnull().sum() / len(df) * 100),\n",
    "                    'Top_Value': df[col].value_counts().index[0] if unique_count > 0 else None,\n",
    "                    'Top_Value_Freq': df[col].value_counts().values[0] if unique_count > 0 else 0\n",
    "                })\n",
    "    \n",
    "    return pd.DataFrame(categorical_analysis).sort_values('Unique_Values')\n",
    "\n",
    "print(\"\\nAnalyzing categorical features...\")\n",
    "categorical_df = analyze_categorical_features(dataset)\n",
    "print(f\"\\nFound {len(categorical_df)} categorical columns (with â‰¤1000 unique values)\")\n",
    "print(\"\\nTop 20 Categorical Features by Cardinality:\")\n",
    "print(categorical_df.nlargest(20, 'Unique_Values')[['File', 'Column', 'Unique_Values', 'Null_%', 'Top_Value']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa3a953",
   "metadata": {},
   "source": [
    "## 5. Numerical Feature Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a72b4e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e12ad5e4",
   "metadata": {},
   "source": [
    "## 6. Feature Engineering Opportunities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8783a2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_feature_engineering_opportunities(data):\n",
    "    opportunities = []\n",
    "    \n",
    "    for fname, df in data.items():\n",
    "        for col in df.columns:\n",
    "            dtype = df[col].dtype\n",
    "            nunique = df[col].nunique()\n",
    "            null_pct = (df[col].isnull().sum() / len(df) * 100)\n",
    "            \n",
    "            recommendations = []\n",
    "            \n",
    "            if dtype == 'object' and nunique <= 50:\n",
    "                recommendations.append('One-Hot Encoding')\n",
    "            \n",
    "            if dtype == 'object' and nunique > 50 and nunique < 1000:\n",
    "                recommendations.append('Target Encoding / Frequency Encoding')\n",
    "            \n",
    "            if pd.api.types.is_numeric_dtype(df[col]) and df[col].min() >= 0:\n",
    "                if df[col].std() > 0:\n",
    "                    recommendations.append('Log Transform / Scaling')\n",
    "            \n",
    "            if 'id' in col.lower() or 'uuid' in col.lower():\n",
    "                recommendations.append('Group Statistics / Aggregation')\n",
    "            \n",
    "            if null_pct > 10:\n",
    "                recommendations.append('Missing Value Imputation')\n",
    "            \n",
    "            if recommendations:\n",
    "                opportunities.append({\n",
    "                    'File': fname,\n",
    "                    'Column': col,\n",
    "                    'Data_Type': str(dtype),\n",
    "                    'Unique_Values': nunique,\n",
    "                    'Missing_%': round(null_pct, 2),\n",
    "                    'Recommendations': ', '.join(recommendations)\n",
    "                })\n",
    "    \n",
    "    return pd.DataFrame(opportunities)\n",
    "\n",
    "print(\"\\n=== Feature Engineering Opportunities ===\")\n",
    "opportunities_df = identify_feature_engineering_opportunities(dataset)\n",
    "print(f\"\\nIdentified {len(opportunities_df)} columns with engineering potential\\n\")\n",
    "for idx, row in opportunities_df.head(30).iterrows():\n",
    "    print(f\"{row['File']:30s} | {row['Column']:20s} | {row['Recommendations']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e0045b",
   "metadata": {},
   "source": [
    "## 7. Correlation & Interaction Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce981e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_correlation_matrix(data):\n",
    "    all_numerical = pd.DataFrame()\n",
    "    \n",
    "    for fname, df in list(data.items())[:5]:\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        if len(numeric_cols) > 0:\n",
    "            print(f\"\\n{fname}:\")\n",
    "            corr_matrix = df[numeric_cols].corr()\n",
    "            \n",
    "            high_corr = []\n",
    "            for i in range(len(corr_matrix.columns)):\n",
    "                for j in range(i+1, len(corr_matrix.columns)):\n",
    "                    if abs(corr_matrix.iloc[i, j]) > 0.7:\n",
    "                        high_corr.append({\n",
    "                            'Var1': corr_matrix.columns[i],\n",
    "                            'Var2': corr_matrix.columns[j],\n",
    "                            'Correlation': round(corr_matrix.iloc[i, j], 3)\n",
    "                        })\n",
    "            \n",
    "            if high_corr:\n",
    "                corr_df = pd.DataFrame(high_corr)\n",
    "                print(corr_df.to_string(index=False))\n",
    "            else:\n",
    "                print(\"No high correlations (>0.7) found\")\n",
    "\n",
    "print(\"\\n=== Correlation Analysis (First 5 Files) ===\")\n",
    "compute_correlation_matrix(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d002e3",
   "metadata": {},
   "source": [
    "## 8. Summary & Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b74216c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FEATURE ENGINEERING SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n1. DATASET OVERVIEW\")\n",
    "print(f\"   - Total Files: {len(dataset)}\")\n",
    "print(f\"   - Total Columns: {sum(len(df.columns) for df in dataset.values())}\")\n",
    "print(f\"   - Total Rows: {sum(len(df) for df in dataset.values()):,}\")\n",
    "print(f\"   - Total Memory: {sum(df.memory_usage(deep=True).sum() for df in dataset.values()) / 1024**2:.2f} MB\")\n",
    "\n",
    "print(f\"\\n2. DATA QUALITY\")\n",
    "avg_duplicates = quality['Duplicates'].mean()\n",
    "avg_missing = quality['Max_Missing_%'].mean()\n",
    "print(f\"   - Average Duplicates per File: {avg_duplicates:.1f}\")\n",
    "print(f\"   - Average Max Missing %: {avg_missing:.2f}%\")\n",
    "\n",
    "print(f\"\\n3. RELATIONSHIPS\")\n",
    "print(f\"   - Total Column Relationships: {len(relationships_df)}\")\n",
    "print(f\"   - Identity Relationships: {len(relationships_df[relationships_df['Type']=='Identity'])}\")\n",
    "print(f\"   - Subset Relationships: {len(relationships_df[relationships_df['Type']=='Subset'])}\")\n",
    "print(f\"   - Intersection Relationships: {len(relationships_df[relationships_df['Type']=='Intersection'])}\")\n",
    "\n",
    "print(f\"\\n4. FEATURE TYPES\")\n",
    "print(f\"   - Categorical Columns: {len(categorical_df)}\")\n",
    "print(f\"   - Numerical Columns: {len(numerical_df)}\")\n",
    "print(f\"   - Temporal Columns: {len(temporal_df)}\")\n",
    "\n",
    "print(f\"\\n5. ENGINEERING OPPORTUNITIES\")\n",
    "print(f\"   - Total Features for Engineering: {len(opportunities_df)}\")\n",
    "print(f\"   - Encoding Candidates: {len(opportunities_df[opportunities_df['Recommendations'].str.contains('Encoding')])}\")\n",
    "print(f\"   - Transformation Candidates: {len(opportunities_df[opportunities_df['Recommendations'].str.contains('Transform')])}\")\n",
    "print(f\"   - Aggregation Candidates: {len(opportunities_df[opportunities_df['Recommendations'].str.contains('Aggregation')])}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
